*** 모듈4 프로젝트 개요 ***
0. 프로젝트 개요 
    1) 아래 4가지 주제 중 하나를 선정하여 데이터를 수집한 다음, 데이터베이스에 저장하는 애플리케이션을 작성해 보시오.
    2) 각 주제에 대해 2022년 1월 ~ 22년 6월까지의 데이터를 일별 또는 주별로 수집하시오.
    3) 주제 선정 후 각 주제별로 수집하려고 하는 데이터 항목을 정리하고 해당 데이터를 수집하시오.
    4) 수집 된 데이터는 csv 파일 또는 json 파일 형태로 저장하며, 데이터베이스 또는 Kafka Cluster를 통하여 저장되도록 하시오.
    5) 수집 된 데이터를 일별 또는 월별로 집계(평균과 같은 기술 통계 적용 -> Pandas 이용 가능)하여 그래프로 출력하시오. 

1. 주제선정
    1) 최신 영화의 리뷰 데이터 
        - 참조 사이트 예: https://www.imdb.com/search/keyword/?keywords=movie-review
        - 수집 데이터 예시) 영화 제목, 평점, 감독, 배우, 개봉일, 장르 등
        - 다른 영화 데이터 가능
    2) 특정 종목의 주식 데이터
        - 참조 사이트: https://finance.naver.com/sise/ 또는 https://finance.yahoo.com/ 
        - 수집 데이터 예시) 현재가, 전일가, 고가, 저가, 시가, 거래량 등
        - 국내 사이트에서 데이터를 수집하기 어려운 경우 해외 사이트에서 가져올 수 있음 (https://finance.yahoo.com/)
    3) 특정 암호화폐 데이터
        - 참조 사이트: https://docs.poloniex.com/#introduction
        - API 상용방법: https://poloniex.com/support/api/ 
        - API 사용 예시) https://poloniex.com/public?command=returnChartData&currencyPair=USDT_BTC&start=1626242746&end=9999999999&period=14400
        - 수집 데이터 예시) 종목명, 현재가, 고가, 저가, 시가, 거래량 등
    4) 특정 지역 날씨 데이터 
        - 참조 사이트: https://openweathermap.org/api (API_KEY를 위해 회원가입 필요)
        - API 사용 예시) https://api.openweathermap.org/data/2.5/weather?q=seoul&appid=[API_KEY] 
        - 수집 데이터 예시) 예측 시간, 날씨, 현재 기온, 최저 기온, 최고 기온, 강수량 등

2. Kakfa Cluster 구축 
    1) 1대의 Zookeeper 서버와 1대의 Kafka Broker를 기동하여 Kafka Cluster 구축
    
3. 데이터 수집 애플리케이션 구현
    1) Python과 Scrapy framework을 이용하여 1번에서 선정한 주제에 대해 데이터를 수집할 수 있는 애플리케이션 개발
    2) Scrapy에 데이터 수집 Spider를 생성하고, 수집하려는 사이트와 데이터를 설정하여 데이터 Cralwing 
    3) 수집 된 데이터는 불필요한 항목을 제거하기 위한 데이터 가공 처리 후, csv 또는 json 파일로 저장
 
4. Scrapy - Kafka 연동 
    1) Scrapy의 Spider에 의해 수집 된 데이터를 Pipelines을 이용하여 Kafka Cluster의 Topic에 저장 (Topic명 자유)
    2) Topic에 저장된 메시지(수집된 데이터)의 내용은 수업 시간에 사용한 Consumer 프로그램이나, Kafka Console Consumer를 이용하여 확인 볼 것

5. Kafka Sink Connect - Mariadb 연동 
    1) Kafka Cluster와 연동할 수 있는 Kafka Connect 설정
    2) Kafka Cluster의 Topic에 저장된 메시지들은 Topic에 저장됨과 동시에 Mariadb에 복사 될 수 있도록 Kafka Sink Connect를 등록
    3) 4번에서 저장된 Topic을 사용하는 Kafka Sink Connect를 생성하기 위한 Script를 생성 (Kafka Connect 수업에 사용한 코드 활용)
    4) Kafka Sink Connect가 정상적으로 작동하는지 확인하기 위해 다음 URL을 활용
        - Connector Plugins 확인 -> GET /connector-plugins/
        - Connectors 확인 -> GET /connectors/
        - 등록 된 Connector 상세 정보 -> GET /connectors/[connecot-name]/status
    5) Kafka Connect에 등록 된 Connector가 정상 작동하게 되면, 지정된 Mariadb에 데이터에 자동으로 생성 됨

6. 데이터 분석 및 시각화 
    1) 수집된 데이터를 일별 또는 월별로 기술적 통계를 처리하고 그래프로 시각화
    2) 기술적 통계의 내용과 그래프 표시 내용 및 방법에 대해서는 자유

** 각 단계별 제출물
1. 프로젝트 개요서 + 주제선정에 대해 요구사항 정리 + 수집할 데이터 목록 정리 등
2. Kafka Cluster에 저장된 Topic의 목록과 Topic에 저장된 메시지들의 내용 (화면 캡쳐)
3. 수집 된 데이터 파일 (csv 또는 json 파일) -> Scrapy 1회 실행 분 중 데이터셋 일부
4. Scrapy project 폴더를 압축하여 제출 
5. Kafka Sink Connect의 Script 파일, Mariadb에 저장된 테이블의 내용 (화면 캡쳐)
6. 데이터 분석한 내용에 대한 그래프 (이미지 파일))

** 제출기한 및 제출방법
1. 위 산출물을 팀(예, 1조_홍길동_이도원.zip)의 이름을 파일명으로 하여 압축(zip 파일)한 다음 압축한 다음, edowon0623@gmail.com 으로 7/26(화)까지 제출
2. 프로젝트 수행에 필요한 인프라는 AWS에 구축한 다음 접속할 수 있는 키(pem)와 아이디/암호를 같이 제출